{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a7b9dd1-0844-405f-b2de-9466b103d8dc",
   "metadata": {},
   "source": [
    "# A Practical Exploration of the Wine Quality Dataset\n",
    "\n",
    "#### By Jordan Cairns, Chris Gao, Yingzi Jin and Chun Li\n",
    "#### In fulfillment of DSCI 522 Milestone 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f2eeab-bdd0-4ab3-9e92-445aa320a2d5",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e804796-518a-4e59-a321-4912ed7f0900",
   "metadata": {},
   "source": [
    "Our analysis aimed to develop a predictive model to distinguish between red and white wines based on various physicochemical properties. This study employed logistic regression, a model renowned for its balance between predictive power and interpretability.\n",
    "\n",
    "The regression result suggested that residual sugar and total sulfur dioxide had high positive coefficients, indicating a strong association with white wine, whereas density showed the most substantial negative impact, followed by alcohol and volatile acidity, suggesting these are key indicators of red wine.\n",
    "\n",
    "\n",
    "The logistic regression model not only achieved high accuracy but also provided valuable insights into the features most indicative of wine type. This model can assist vintners in quality control and classification tasks. Moreover, the interpretability of the model offers a foundation for further research into wine composition and its impact on sensory attributes. Future studies might explore more complex models or delve deeper into feature engineering to enhance predictive accuracy and understanding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c56df3c-6df6-4bbc-b9aa-7c76fd447f1d",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e40f11a-dc37-4257-9d0e-345558ae2014",
   "metadata": {},
   "source": [
    "In the intricate world of oenology, the distinction between red and white wines extends beyond color, embedding itself in the nuanced spectrum of their physicochemical properties. This project embarks on a data-driven journey to unravel these complexities by leveraging statistical models to classify wines as red or white based on their inherent characteristics. Utilizing a rich dataset that encapsulates key attributes like acidity, sugar content, sulfur dioxide levels, alcohol concentration, and more, we aim to build a predictive model that not only accurately classifies the wines but also sheds light on the influential factors that underpin this classification. Through this analysis, we intend to blend the art of winemaking with the precision of data science, offering insights that could prove valuable to vintners, sommeliers, and wine enthusiasts alike in understanding the subtle distinctions between these two celebrated categories of wine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba2e24-9e5d-41ad-8c1c-99270d18d833",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe11e697-7cd6-4fb3-aaa1-906dcf25a113",
   "metadata": {},
   "source": [
    "The dataset utilized in our project is sourced from the UCI Machine Learning Repository, specifically focusing on red and white variants of Portuguese \"Vinho Verde\" wine​​​​. This dataset is distinguished by its emphasis on physicochemical tests to model wine quality, capturing a range of variables that reflect the sensory and chemical composition of the wine samples. Notably, it encompasses various input features like acidity, sugar content, and alcohol levels, while the output variable relates to the sensory-driven quality rating of the wines. A unique aspect of this dataset is its exclusion of data on grape types, wine brands, or prices due to privacy and logistic constraints. This attribute frames our analysis within a context of physicochemical and sensory data, offering an opportunity to delve into wine quality assessment based on measurable attributes, free from commercial biases. The dataset's structure lends itself to both classification and regression tasks, providing a fertile ground for exploring machine learning applications in the domain of wine quality evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff81cd71-7570-46ca-a1d2-2709cfec4e4c",
   "metadata": {},
   "source": [
    "## Data Overview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "862b6a32-a7ff-4d92-9776-832edd3960af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import altair as alt\n",
    "import os\n",
    "import sys\n",
    "from myst_nb import glue\n",
    "import pickle\n",
    "\n",
    "from sklearn import set_config\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "sys.path.insert(0, \"../src\")\n",
    "from helper_func_wine_classification_plot import create_wine_prediction_chart "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "89f3acef-80f0-4157-b76f-b8e6d7829594",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_combined = pd.read_csv(\"../data/raw/winequality.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f703b99e-c756-485e-8630-569d6b6b81a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnp.random.seed(522)\\nset_config(transform_output=\"pandas\")\\n\\n# Creating the split\\nwine_train, wine_test = train_test_split(df_wine, train_size=0.70, stratify=df_wine[\"color\"])\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "np.random.seed(522)\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "# Creating the split\n",
    "wine_train, wine_test = train_test_split(df_wine, train_size=0.70, stratify=df_wine[\"color\"])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149b1aa2-004d-44f2-9a8d-4be1784aedee",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18913b9c-9607-4da3-9f8a-f25ca57bd778",
   "metadata": {},
   "source": [
    "The first step of EDA is to generate some histograms to visualize the effects of all numerical variables to the type of wines. By comparing these distributions side by side, we can pinpoint which features exhibit significant variations across the two categories, thereby informing feature selection for predictive modeling. Such visual tools are invaluable as they facilitate an intuitive understanding of complex data relationships, highlight potential factors that could influence the wine's classification, and guide subsequent analytical steps in the data science workflow.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![comparitive_distribution](../results/figures/comparative_distribution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![numeric_histogram_plots](../results/figures/numeric_histogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3e4586-f2eb-4010-8b4c-36cb90c4b664",
   "metadata": {},
   "source": [
    "Visually, some features do show significant differences between red and white wines and may be particularly relevant in distinguishing between the two. In particular, the following five features stand out in the histograms and could be considered significant for predicting the color of the wine.\n",
    "\n",
    "1. Fixed & Volatile Acidity: There's a noticeable difference in the distributions, with red wines generally exhibiting higher fixed and volatile acidity.\n",
    "\n",
    "2. Residual Sugar: White wines display a much higher residual sugar content, which could be a strong differentiator\n",
    "\n",
    "3. Total Sulfur Dioxide: The levels are significantly higher in white wines, suggesting this feature could be key in classification.\n",
    "\n",
    "4. Free Sulfur Dioxide: Similar to total sulfur dioxide, this feature is also markedly higher in white wines.\n",
    "\n",
    "5. pH value: The majority of red wines seem to have a higher overall pH values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![corr_plot](../results/figures/correlation_matrix_heatmap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fae2ffa",
   "metadata": {},
   "source": [
    "The distribution of the plot also demonstrates the majority of the explainatory variables are not strongly corrlated. However, we do observe the correlations between variable pairs `free sulfur dioxide` and `total sulfur dioxide`, as well as `density` and `alcohol` are relatively high (absolute value exceeding 0.7). This might introduce difficulties to the model to estimate the relationship between each independent variable and the dependent variable independently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2247c6-0367-4616-8b8c-ad6e34dcdf8e",
   "metadata": {},
   "source": [
    "## Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b9fc77ba-a2a3-4af8-bc40-01ad9fff1aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Build a transformer to further process the data\\nnumeric_features = wine_train.columns.tolist()[:-2]\\ncategorical_features = wine_train.columns.tolist()[-2:-1]\\ncolumns_to_passthrough = [\\'color\\']\\n\\nwine_preprocessor = make_column_transformer(\\n    (StandardScaler(), numeric_features),\\n    (OrdinalEncoder(), categorical_features),\\n    (\\'passthrough\\', columns_to_passthrough)\\n)\\n\\nwine_preprocessor.fit(wine_train)\\nscaled_wine_train = wine_preprocessor.transform(wine_train)\\nscaled_wine_test = wine_preprocessor.transform(wine_test)\\n\\nscaled_wine_train.to_csv(\"../data/processed/scaled_wine_train.csv\")\\nscaled_wine_test.to_csv(\"../data/processed/scaled_wine_test.csv\") '"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Build a transformer to further process the data\n",
    "numeric_features = wine_train.columns.tolist()[:-2]\n",
    "categorical_features = wine_train.columns.tolist()[-2:-1]\n",
    "columns_to_passthrough = ['color']\n",
    "\n",
    "wine_preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), numeric_features),\n",
    "    (OrdinalEncoder(), categorical_features),\n",
    "    ('passthrough', columns_to_passthrough)\n",
    ")\n",
    "\n",
    "wine_preprocessor.fit(wine_train)\n",
    "scaled_wine_train = wine_preprocessor.transform(wine_train)\n",
    "scaled_wine_test = wine_preprocessor.transform(wine_test)\n",
    "\n",
    "scaled_wine_train.to_csv(\"../data/processed/scaled_wine_train.csv\")\n",
    "scaled_wine_test.to_csv(\"../data/processed/scaled_wine_test.csv\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e209885-139d-4b4c-b49b-a912f897c377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" # Rename 'passthrough__color' back to 'color'\\nscaled_wine_train.rename(columns={'passthrough__color': 'color'}, inplace=True)\\nscaled_wine_test.rename(columns={'passthrough__color': 'color'}, inplace=True)\\n\\n# Preparing data for machine learning model\\nX_train = scaled_wine_train.drop(columns=['color'])\\ny_train = scaled_wine_train['color']\\n\\nX_test = scaled_wine_test.drop(columns=['color'])\\ny_test = scaled_wine_test['color'] \""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Rename 'passthrough__color' back to 'color'\n",
    "scaled_wine_train.rename(columns={'passthrough__color': 'color'}, inplace=True)\n",
    "scaled_wine_test.rename(columns={'passthrough__color': 'color'}, inplace=True)\n",
    "\n",
    "# Preparing data for machine learning model\n",
    "X_train = scaled_wine_train.drop(columns=['color'])\n",
    "y_train = scaled_wine_train['color']\n",
    "\n",
    "X_test = scaled_wine_test.drop(columns=['color'])\n",
    "y_test = scaled_wine_test['color'] \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3cb165a7-29ee-4508-89a8-f0d2d7a707d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Creating the DummyClassifier to get the baseline score\\ndummy_scores = pd.DataFrame(cross_validate(\\n    DummyClassifier(strategy=\"most_frequent\"),\\n    X_train,\\n    y_train,\\n    return_train_score=True,\\n    scoring=[\"accuracy\"]\\n))\\n\\ndummy_scores '"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Creating the DummyClassifier to get the baseline score\n",
    "dummy_scores = pd.DataFrame(cross_validate(\n",
    "    DummyClassifier(strategy=\"most_frequent\"),\n",
    "    X_train,\n",
    "    y_train,\n",
    "    return_train_score=True,\n",
    "    scoring=[\"accuracy\"]\n",
    "))\n",
    "\n",
    "dummy_scores \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8021563e-5e7c-405b-b820-03ba3eabef0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from helper_func_model_selection import model_selection\\nmodels = model_selection(\"dummy\", \"dtree\", \"knn\", \"svm\", \"nb\", \"lr\")\\nmodels '"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from helper_func_model_selection import model_selection\n",
    "models = model_selection(\"dummy\", \"dtree\", \"knn\", \"svm\", \"nb\", \"lr\")\n",
    "models \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "79a71eba-7529-4e78-8243-adfdf53a3bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # The following block of code was inspired by DSCI 571 Lab 4\\n\\nresults_list = []\\n\\nfor name, model in models.items():\\n    \\n    # Create a pipeline with a CountVectorizer and the current model\\n    pipeline = make_pipeline(model)\\n    \\n    # Perform cross-validation\\n    cv_results = cross_validate(pipeline, X_train, y_train, cv=5,\\n    return_train_score=True,\\n    scoring=\\'accuracy\\',\\n    n_jobs=-1)\\n    \\n    # Append results for the current model to the results_list\\n    results_list.append({\\n        \"model\": name,\\n        \"fit_time\": np.mean(cv_results[\\'fit_time\\']),\\n        \"score_time\": np.mean(cv_results[\\'score_time\\']),\\n        \"test_score\": np.mean(cv_results[\\'test_score\\']),\\n        \"train_score\": np.mean(cv_results[\\'train_score\\']),\\n    })\\n\\n# Create a DataFrame from the results_list\\nresults_df = pd.DataFrame(results_list)\\n\\n# Set the model name as the index\\nresults_df.set_index(\\'model\\', inplace=True)\\n\\n# Show the resulting DataFrame\\nresults_df '"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # The following block of code was inspired by DSCI 571 Lab 4\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    \n",
    "    # Create a pipeline with a CountVectorizer and the current model\n",
    "    pipeline = make_pipeline(model)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = cross_validate(pipeline, X_train, y_train, cv=5,\n",
    "    return_train_score=True,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1)\n",
    "    \n",
    "    # Append results for the current model to the results_list\n",
    "    results_list.append({\n",
    "        \"model\": name,\n",
    "        \"fit_time\": np.mean(cv_results['fit_time']),\n",
    "        \"score_time\": np.mean(cv_results['score_time']),\n",
    "        \"test_score\": np.mean(cv_results['test_score']),\n",
    "        \"train_score\": np.mean(cv_results['train_score']),\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the results_list\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Set the model name as the index\n",
    "results_df.set_index('model', inplace=True)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "results_df \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2898ca57-c54f-4bd7-9cc3-f4535415c69d",
   "metadata": {},
   "source": [
    "Upon concluding our exploratory data analysis and delving into model evaluation, the results delineate an intriguing landscape of model performance. Notably, while the Decision Tree, KNN, and RBF SVM models exhibit high accuracy, with the SVM model achieving the highest test scores, the choice of model cannot rest on accuracy alone. Logistic Regression, while marginally surpassed by SVM in test score metrics, stands out for its interpretability. This model provides not only a robust predictive performance but also the capacity to glean meaningful insights from the significance and impact of each feature, as reflected by its coefficients. In light of this, we opt for Logistic Regression, valuing the interpretative clarity it offers, which is instrumental for a nuanced understanding of the variables influencing wine classification. This strategic choice harmonizes predictive strength with explanatory depth, guiding us towards actionable intelligence over mere predictive prowess\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1a4e823d-c129-4423-a6de-3e2ae2b1dda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Fitting the Logistic Regression model and score it on the test portion\\nmodel = LogisticRegression(max_iter=1000)\\nmodel.fit(X_train,y_train)\\nmodel.score(X_test, y_test) '"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Fitting the Logistic Regression model and score it on the test portion\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test, y_test) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f847a8bd-42fe-4731-b9d3-b90950113307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" # Producing the table to present the marginal contribution of each feature.\\nreg_data = {\\n    'Feature Name': model.feature_names_in_,\\n    'Coefficient': model.coef_[0]\\n}\\n\\nresult_df = pd.DataFrame(reg_data)\\n\\nresult_df['Feature Name'] = result_df['Feature Name'].str.replace('standardscaler__', '')\\nresult_df['Feature Name'] = result_df['Feature Name'].str.replace('ordinalencoder__', '')\\n\\nresult_df \""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Producing the table to present the marginal contribution of each feature.\n",
    "reg_data = {\n",
    "    'Feature Name': model.feature_names_in_,\n",
    "    'Coefficient': model.coef_[0]\n",
    "}\n",
    "\n",
    "result_df = pd.DataFrame(reg_data)\n",
    "\n",
    "result_df['Feature Name'] = result_df['Feature Name'].str.replace('standardscaler__', '')\n",
    "result_df['Feature Name'] = result_df['Feature Name'].str.replace('ordinalencoder__', '')\n",
    "\n",
    "result_df \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eaf58a48-3517-4042-bb19-51d4e9df89a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'[wine_prediction_chart]' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "![wine_prediction_chart](../results/figures/predict_visualization.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ae26db58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwine_train.head(5).to_csv(\"../data/test/test_data_alt_distri.cvs\")\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "wine_train.head(5).to_csv(\"../data/test/test_data_alt_distri.cvs\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1bd80d-cf66-448c-aebe-bb709cd158ac",
   "metadata": {},
   "source": [
    "The coefficients obtained from the logistic regression model provide a quantifiable measure of the impact each feature has on the likelihood of a wine being classified as red or white. Features with positive coefficients, such as residual sugar and total sulfur dioxide, increase the probability of a wine being classified as white, as indicated by the model.classes_ array. Conversely, features with negative coefficients, such as alcohol, volatile acidity, chlorides, and notably density with the largest negative coefficient, are indicative of a wine being classified as red. The magnitude of these coefficients reveals the relative importance of each feature, with density and alcohol having the most substantial influence in the negative direction and residual sugar significantly increases the odds in favor of white wine. The feature 'quality' also plays a role, albeit a smaller one, in swaying the classification towards red wine. Overall, the model's coefficients provide a nuanced understanding of how each physicochemical characteristic tilts the balance in the complex interplay of factors that determine wine color in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c276f545-7fc5-4d43-a130-41bfe2866ecb",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b319fa17-380a-4f69-8d73-7536cf043fa3",
   "metadata": {},
   "source": [
    "The logistic regression analysis reveals expected relationships between wine characteristics and their classification as red or white. Residual sugar's positive coefficient aligns with the higher levels typically found in white wines, indicating a greater likelihood of a wine being classified as white as the sugar content increases. Similarly, the positive coefficient for sulfur dioxide corresponds with the higher concentrations in white wines. The negative coefficients for alcohol and density suggest a higher probability of wine being classified as red with increasing values, which is consistent with red wines generally having higher alcohol content. These insights highlight the intricate balance of physicochemical properties influencing wine color, reaffirming the importance of considering the context and interactions of features within the dataset when interpreting model outcomes.\n",
    "\n",
    "\n",
    "\n",
    "Nevertheless, it's important to remember that the signs and magnitudes of coefficients in logistic regression are influenced by the scale of the features and the correlations between them. These factors can affect the interpretability of the coefficients in complex ways, especially if there is multicollinearity in the data. Therefore, while the results are plausible and show some expected trends, any surprising findings would warrant a deeper investigation into the data and the model's behavi \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0735ff-1d8e-42a8-bbd0-f36c1ab11f91",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1e96e1-99cf-4003-aa49-5be04962d1ae",
   "metadata": {},
   "source": [
    "Cortez, P., Cerdeira, A., Almeida, F., Matos, T., & Reis, J. (2009). Wine Quality Dataset. UCI Machine Learning Repository. Retrieved from https://archive.ics.uci.edu/dataset/186/wine+quality\n",
    "\n",
    "Timbers, T. (2023). Breast Cancer Predictor Python Repository. GitHub. Retrieved from https://github.com/ttimbers/breast_cancer_predictor_py/tree/v0.0.2\n",
    "\n",
    "Mor, N. S. (2022).Wine Quality and Type Prediction from Physicochemical Properties Using Neural Networks for Machine Learning: A Free Software for Winemakers and Customer. https://osf.io/ph4cu/download.\n",
    "\n",
    "UBC Master of Data Science. (2023). DSCI 571: Supervised Learning I. UBC GitHub. Retrieved from https://github.ubc.ca/MDS-2023-24/DSCI_571_sup-learn-1_students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee4b58e-1297-4308-bcd7-5ffbc83e122d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "group_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
